{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e65228",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"llama-chat-finetune.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "         <a>2</a>\n",
    "        <a href=\"triton-llama.ipynb\">3</a>\n",
    "        <a href=\"challenge.ipynb\">4</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"triton-llama.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebbdbc0-1287-45a7-a6d7-6d0cb489eb0f",
   "metadata": {},
   "source": [
    "# Building TensorRT Engine With Finetune Model  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55304b-8d44-400e-b3df-8fbce1f0ce48",
   "metadata": {},
   "source": [
    "The objective of this nootebook is to demostrate the use of TensorRT-LLM to optimizing our finetune Llama-2-7b-chat (`../../model/Llama-2-7b-chat-hf-merged`) from the previous notebook, run inference, and examine using various advance optimization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b3aaa9-4cf5-4ea9-af9e-f3f5e85aa8c2",
   "metadata": {},
   "source": [
    "### Overview of TensorRT-LLM \n",
    "\n",
    "[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/main) is a toolkit to assemble optimized solutions to perform Large Language Model (LLM) inference. It offers a Python API to define models and compile efficient [TensorRT](https://developer.nvidia.com/tensorrt) engines for NVIDIA GPUs. It also contains Python and C++ components to build runtimes to execute those engines as well as [backends](https://github.com/triton-inference-server/tensorrtllm_backend) for the [Triton Inference Server](https://developer.nvidia.com/triton-inference-server) to easily create web-based services for LLMs. TensorRT-LLM supports single GPU, multi-GPU and multi-node configurations (using [Tensor Parallelism](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/parallelisms.html#tensor-parallelism) and/or [Pipeline Parallelism](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/parallelisms.html#pipeline-parallelism)). TensorRT-LLM wraps TensorRT’s deep learning compiler—which includes optimized kernels from FasterTransformer, pre- and post-processing, and multi-GPU and multi-node communication—in a simple open-source Python API for defining, optimizing, and executing LLMs for inference in production.\n",
    "\n",
    "The Python API of TensorRT-LLM is architectured to look similar to the PyTorch API. It provides users with a functional module containing functions like `einsum`, `softmax`, `matmul` or `view`. TensorRT-LLM maximize performance and reduce memory footprint by allowing models to be execute using different quantization modes. Thus, it supports INT4 or INT8 weights (and FP16 activations; a.k.a. INT4/INT8 weight-only) as well as a complete implementation of the [SmoothQuant technique](https://arxiv.org/abs/2211.10438)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c4bc0-b7e8-414e-837b-25a94f7d4ee0",
   "metadata": {},
   "source": [
    "### Key Features of TensorRT-LLM\n",
    "\n",
    "TensorRT-LLM contains examples that implement the following features.\n",
    "\n",
    "* Multi-head Attention([MHA](https://arxiv.org/abs/1706.03762))\n",
    "* Multi-query Attention ([MQA](https://arxiv.org/abs/1911.02150))\n",
    "* Group-query Attention([GQA](https://arxiv.org/abs/2307.09288))\n",
    "* In-flight Batching\n",
    "* Paged KV Cache for the Attention\n",
    "* Tensor Parallelism\n",
    "* Pipeline Parallelism\n",
    "* INT4/INT8 Weight-Only Quantization (W4A16 & W8A16)\n",
    "* [SmoothQuant](https://arxiv.org/abs/2211.10438)\n",
    "* [GPTQ](https://arxiv.org/abs/2210.17323)\n",
    "* [AWQ](https://arxiv.org/abs/2306.00978)\n",
    "* [FP8](https://arxiv.org/abs/2209.05433)\n",
    "* Greedy-search\n",
    "* Beam-search\n",
    "* RoPE\n",
    "\n",
    "Some of the features are not enabled for these [models](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples). Please find list of TensorRT-LLM supported models [here](https://github.com/NVIDIA/TensorRT-LLM/tree/main?tab=readme-ov-file#models)\n",
    "\n",
    "### Support Device\n",
    "\n",
    "TensorRT-LLM is rigorously tested on the following GPUs:\n",
    "\n",
    "- H100\n",
    "- L40S\n",
    "- A100\n",
    "- A30\n",
    "- V100 (experimental)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52330293-ffeb-4b4d-92d2-2d74797d5364",
   "metadata": {},
   "source": [
    "## Building TensorRT engine(s) for Llama-2\n",
    "\n",
    "In this section, we show how to build tensorrt engine(s) using our merged model. Firstly, we use the `build.py` script to build our tensorrt engine using single GPU and FP16. The second step is to run the inference using the `run.py` script.  Before we proceed to buid our engine, it important to be aware of the supported matrixes for Llama-2 as listed bellow:\n",
    "\n",
    "- FP16\n",
    "- FP8\n",
    "- INT8 & INT4 Weight-Only\n",
    "- SmoothQuant\n",
    "- Groupwise quantization (AWQ/GPTQ)\n",
    "- FP8 KV CACHE\n",
    "- INT8 KV CACHE (+ AWQ/per-channel weight-only)\n",
    "- Tensor Parallel\n",
    "- STRONGLY TYPED\n",
    "\n",
    "**flag description**:\n",
    "- **model_dir**: path to the model directory \n",
    "- **output_dir**: path to the directory to store the tensorrt-llm checkpoint format or the tensorrt engine\n",
    "- **dtype**:  data type to use in for model covertion to tensorrt-llm checkpoint\n",
    "- **checkpoint_dir**: path to the directory to load the tensorrt-llm checkpoint needed to build the tensorrt engine\n",
    "- **use_gemm_plugin**: required plugin to prevent accuracy issue\n",
    "- **gpt_attention_plugin**: GPT attention plugin\n",
    "- **weight_only_precision**: required weight precision to build tensorrt engine\n",
    "- **enable_context_fmha**: context-dependent Faster Multihead Attention plugin that reduce the memory footprint significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe1c292-aac8-46a9-835c-c1a6029305da",
   "metadata": {},
   "source": [
    "#### Build the LLaMA 7B model using a single GPU and FP16 \n",
    "\n",
    "- Build Tensorrt Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f8417-4e6d-472c-8074-84d4a29df5ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3  ../../../tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\\n",
    "                --model_dir ../../model/Llama-2-7b-chat-hf-merged \\\n",
    "                --dtype float16 \\\n",
    "                --remove_input_padding \\\n",
    "                --use_gpt_attention_plugin float16 \\\n",
    "                --enable_context_fmha \\\n",
    "                --use_gemm_plugin float16 \\\n",
    "                --output_dir ../../model/trt_engines/fp16/1-gpu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba01f0-2bbe-463b-9ed6-c75a79f187bf",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "```python\n",
    "...\n",
    "[02/23/2024-22:04:07] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +12853, now: CPU 0, GPU 12853 (MiB)\n",
    "[02/23/2024-22:04:11] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 32216 MiB\n",
    "[02/23/2024-22:04:11] [TRT-LLM] [I] Total time of building llama_float16_tp1_rank0.engine: 00:00:35\n",
    "[02/23/2024-22:04:11] [TRT-LLM] [I] Config saved to ../../model/trt_engines/fp16/1-gpu/config.json.\n",
    "[02/23/2024-22:04:11] [TRT-LLM] [I] Serializing engine to ../../model/trt_engines/fp16/1-gpu/llama_float16_tp1_rank0.engine...\n",
    "[02/23/2024-22:04:26] [TRT-LLM] [I] Engine serialized. Total time: 00:00:14\n",
    "[02/23/2024-22:04:26] [TRT-LLM] [I] Timing cache serialized to ../../model/trt_engines/fp16/1-gpu/model.cache\n",
    "[02/23/2024-22:04:26] [TRT-LLM] [I] Total time of building all 1 engines: 00:01:22\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8483ace4-5edc-47d2-bd6a-1947ae5afcfa",
   "metadata": {},
   "source": [
    "- Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57bad3-50af-44cc-b1ba-b8ed752e4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3  ../../../tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\\n",
    "               --max_output_len=200 \\\n",
    "               --tokenizer_dir ../../model/Llama-2-7b-chat-hf-merged \\\n",
    "               --engine_dir=../../model/trt_engines/fp16/1-gpu/ \\\n",
    "               --input_text \"explain what is astrophotography?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f668445-fab3-4b39-ad71-34823b121cf9",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "Running the float16 engine ...\n",
    "Input: \"explain what is astrophotography?\"\n",
    "Output: \"\n",
    "\n",
    "Astrophotography is the branch of photography that deals with the capture of images of celestial objects such as stars, planets, and galaxies. It involves the use of specialized equipment such as telescopes, cameras, and lenses to capture images of these objects. Astrophotography is a challenging and rewarding field that requires a combination of technical knowledge and creativity.\n",
    "\n",
    "Astrophotography can be used to study the properties of celestial objects, such as their size, shape, and color. It can also be used to create artistic images that capture the beauty and wonder of the night sky. Astrophotography has been used to capture some of the most iconic images of the universe, including the Pillars of Creation in the Eagle Nebula and the Orion Nebula.\n",
    "\n",
    "Astrophotography is a popular hobby and profession, with many amateur astronomers and\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94fac36-e503-4877-bcb8-2ac0f0e86aee",
   "metadata": {},
   "source": [
    "#### Build the LLaMA 7B model using a single GPU and BF16\n",
    "\n",
    "- Build Tensorrt Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc1d9b-b259-4808-be73-9e59d4d073e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3  ../../../tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\\n",
    "                --model_dir ../../model/Llama-2-7b-chat-hf-merged \\\n",
    "                --dtype bfloat16 \\\n",
    "                --remove_input_padding \\\n",
    "                --use_gpt_attention_plugin bfloat16 \\\n",
    "                --enable_context_fmha \\\n",
    "                --use_gemm_plugin bfloat16 \\\n",
    "                --output_dir ../../model/trt_engines/bf16/1-gpu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c1370-23d0-49e1-8e02-d7ed8866653e",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```python\n",
    "...\n",
    "[02/23/2024-22:15:37] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 42037 MiB\n",
    "[02/23/2024-22:15:37] [TRT-LLM] [I] Total time of building llama_bfloat16_tp1_rank0.engine: 00:00:29\n",
    "[02/23/2024-22:15:37] [TRT-LLM] [I] Config saved to ../../model/trt_engines/bf16/1-gpu/config.json.\n",
    "[02/23/2024-22:15:37] [TRT-LLM] [I] Serializing engine to ../../model/trt_engines/bf16/1-gpu/llama_bfloat16_tp1_rank0.engine...\n",
    "[02/23/2024-22:15:52] [TRT-LLM] [I] Engine serialized. Total time: 00:00:15\n",
    "[02/23/2024-22:15:52] [TRT-LLM] [I] Timing cache serialized to ../../model/trt_engines/bf16/1-gpu/model.cache\n",
    "[02/23/2024-22:15:53] [TRT-LLM] [I] Total time of building all 1 engines: 00:00:59\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612dd1f1-bfec-403f-9049-9e05cd6e0c28",
   "metadata": {},
   "source": [
    "- Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5092f730-2cfd-467b-ba8b-d888db97f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3  ../../../tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\\n",
    "               --max_output_len=200 \\\n",
    "               --tokenizer_dir ../../model/Llama-2-7b-chat-hf-merged \\\n",
    "               --engine_dir=../../model/trt_engines/bf16/1-gpu/ \\\n",
    "               --input_text \"explain what is astrophotography?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f77a8f-2eb5-4497-a7ab-e3e7bff4801b",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```python\n",
    "\n",
    "Running the bfloat16 engine ...\n",
    "Input: \"explain what is astrophotography?\"\n",
    "Output: \"\n",
    "\n",
    "Astrophotography is the branch of photography that deals with the capture of images of celestial objects such as stars, planets, and galaxies. It involves the use of specialized equipment such as telescopes, cameras, and lenses to capture images of these objects. Astrophotography is a challenging and rewarding field that requires a combination of technical knowledge and creativity.\n",
    "\n",
    "Astrophotography can be used to study the properties of celestial objects, such as their size, shape, and color. It can also be used to create artistic images that capture the beauty and wonder of the night sky. Astrophotography has been used to capture some of the most iconic images of the universe, including the Pillars of Creation in the Eagle Nebula and the Orion Nebula.\n",
    "\n",
    "Astrophotography is a popular hobby and profession, with many amateur astronomers and\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1341f3-d5cb-4409-90bb-3631bc0756e5",
   "metadata": {},
   "source": [
    "#### Build the LLaMA 7B model using a single GPU and apply INT8 weight-only quantization\n",
    "\n",
    "- Build Tensorrt Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec3b34-1271-4fb7-ae76-4246c112b8ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3  ../../../tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\\n",
    "                --model_dir ../../model/Llama-2-7b-chat-hf-merged \\\n",
    "                --dtype float16 \\\n",
    "                --remove_input_padding \\\n",
    "                --use_gpt_attention_plugin float16 \\\n",
    "                --enable_context_fmha \\\n",
    "                --use_gemm_plugin float16 \\\n",
    "                --use_weight_only \\\n",
    "                --output_dir ../../model/trt_engines/weight_only/1-gpu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f83231-b307-4e78-a092-c39d38a33451",
   "metadata": {},
   "source": [
    "Expected Output:\n",
    "\n",
    "```python\n",
    "...\n",
    "[02/23/2024-22:23:17] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 29558 MiB\n",
    "[02/23/2024-22:23:17] [TRT-LLM] [I] Total time of building llama_float16_tp1_rank0.engine: 00:00:26\n",
    "[02/23/2024-22:23:17] [TRT-LLM] [I] Config saved to ../../model/trt_engines/weight_only/1-gpu/config.json.\n",
    "[02/23/2024-22:23:17] [TRT-LLM] [I] Serializing engine to ../../model/trt_engines/weight_only/1-gpu/llama_float16_tp1_rank0.engine...\n",
    "[02/23/2024-22:23:25] [TRT-LLM] [I] Engine serialized. Total time: 00:00:07\n",
    "[02/23/2024-22:23:25] [TRT-LLM] [I] Timing cache serialized to ../../model/trt_engines/weight_only/1-gpu/model.cache\n",
    "[02/23/2024-22:23:25] [TRT-LLM] [I] Total time of building all 1 engines: 00:02:48\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc060b3-5dfe-4317-8e5b-ac1289c81a68",
   "metadata": {},
   "source": [
    "- Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc341721-043d-4faf-b671-825f21192440",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3  ../../../tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\\n",
    "               --max_output_len=200 \\\n",
    "               --tokenizer_dir ../../model/Llama-2-7b-chat-hf-merged \\\n",
    "               --engine_dir=../../model/trt_engines/weight_only/1-gpu/ \\\n",
    "               --input_text \"explain what is astrophotography?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b06153-d81b-469e-a1dc-8f831d5a432b",
   "metadata": {},
   "source": [
    "Expected Output\n",
    "\n",
    "```python\n",
    "Running the float16 engine ...\n",
    "Input: \"explain what is astrophotography?\"\n",
    "Output: \"\n",
    "\n",
    "Astrophotography is the branch of photography that deals with the capture of images of celestial objects such as stars, planets, and galaxies. It involves the use of specialized equipment such as telescopes, cameras, and lenses to capture images of these objects. Astrophotography is a challenging and rewarding field that requires a combination of technical knowledge and creativity.\n",
    "\n",
    "Astrophotography can be used to study the properties of celestial objects, such as their size, shape, and color. It can also be used to create artistic images that capture the beauty and wonder of the night sky. Astrophotography has been used to capture some of the most iconic images of the universe, including the Pillars of Creation in the Eagle Nebula and the Orion Nebula.\n",
    "\n",
    "Astrophotography is a popular hobby and profession, with many amateur astronomers and\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defddd32-ef03-4cb3-a176-67b4b9c9ca0c",
   "metadata": {},
   "source": [
    "#### Other methods to Build and Run LLaMA-2 7B, 30B, and 70B\n",
    "\n",
    "- 2-way tensor parallelism.\n",
    "- 2-way tensor parallelism and 2-way pipeline parallelism\n",
    "- 8-way tensor parallelism for 70B\n",
    "- 4-way tensor parallelism and 2-way pipeline parallelism for 70B\n",
    "- Build LLaMA 70B TP=8 using Meta checkpoints directly.\n",
    "\n",
    "Please find examples for the listed methods [here](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2d686-ac1b-44b7-8503-cb6dd5df4412",
   "metadata": {},
   "source": [
    "### Advance OPtimization Techniques\n",
    "\n",
    "- **Quantization** \n",
    "\n",
    "TensorRT-LLM implements different quantization methods with support matrix for the different models. Given a matrix (2D tensor) of shape M x N (M rows and N columns) where M is the number of tokens and N is the number of channels. TensorRT-LLM has the three following modes to quantize and dequantize the elements of the tensor:\n",
    "\n",
    "    - Per-tensor: It uses a single scaling factor for all the elements,\n",
    "    - Per-token: It uses a different scaling factor for each token. There are M scaling factors in that case,\n",
    "    - Per-channel: It uses a different scaling factor for each channel. There are N scaling factors in that case.\n",
    "\n",
    "```python\n",
    "\n",
    "# Per-tensor scaling.\n",
    "for mi in range(M):\n",
    "    for ni in range(N):\n",
    "        q[mi][ni] = int8.satfinite(x[mi][ni] * s)\n",
    "\n",
    "# Per-token scaling.\n",
    "for mi in range(M):\n",
    "    for ni in range(N):\n",
    "        q[mi][ni] = int8.satfinite(x[mi][ni] * s[mi])\n",
    "\n",
    "# Per-channel scaling.\n",
    "for mi in range(M):\n",
    "    for ni in range(N):\n",
    "        q[mi][ni] = int8.satfinite(x[mi][ni] * s[ni])\n",
    "\n",
    "```\n",
    "Use the [link](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/precision.md) to explore more topics that include INT8 SmoothQuant, INT4 and INT8 Weight-Only, GPTQ, and AWQ.\n",
    "\n",
    "- **In-flight Batching**\n",
    "\n",
    "In-flight Batching also known as continuous batching or iteration-level batching. The technique aims at reducing wait times in queues, eliminating the need for padding requests and allowing for higher GPU utilization. TensorRT-LLM uses on a component, called the Batch Manager, to support in-flight batching of requests. More on The Batch Manager API can be found [here](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/batch_manager.md) \n",
    "\n",
    "- **Multi-head, Multi-query and Group-query Attention**\n",
    "\n",
    "Multi-head(MHA), Multi-query(MQA) and Group-query Attention(GQA) are variants of attention mechanism found in most the Large Language Models, and are implemented and optimized in TensorRT-LLM. The [MHA](https://arxiv.org/abs/1706.03762) is the sequence of a batched matmul, a softmax and another batched matmul while [MQA](https://arxiv.org/abs/1911.02150) and [GQA](https://arxiv.org/abs/2307.09288) are variants of MHA that use fewer, so-called, K/V head than the number of query heads.. This [document](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/gpt_attention.md) summarizes those implementations in TensorRT-LLM.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36a92f-720a-47b6-9b5b-bc8a98c33038",
   "metadata": {},
   "source": [
    "## Performance of TensorRT-LLM\n",
    "\n",
    "\n",
    "The data in the following tables is provided as a reference point to help users validate observed performance for TensorRT-LLM on H100 (Hopper). It should not be considered as the peak performance that can be delivered by TensorRT-LLM. The different performance numbers below were collected using single GPU, a single node with multiple GPUs or multiple nodes with multiple GPUs for GPT, GPT-like(LLaMA/OPT/GPT-J/SmoothQuant-GPT), BERT models, and Encoder-Decoder models for Peak Throughput and Low Latency. Below is the table for H100 GPUs (FP8).\n",
    " \n",
    " \n",
    " | Model | Batch Size | TP (1) | Input Length | Output Length |  Throughput (out tok/s/GPU) |\n",
    " |:-----:|:--------:| :-------------: | :------------: | :------------: | :--------------: |\n",
    " |GPT-J 6B|\t1024|1|128|128|26,150|\n",
    " |GPT-J 6B|\t120\t|   1|128  |   2048|8,011|\n",
    " |GPT-J 6B|\t64\t|1\t |2048 |    128|2,551|\n",
    " |GPT-J 6B|\t64\t|1\t |2048 |   2048|3,327|\n",
    " |LLaMA 7B|\t768\t|1\t |128  |    128|19,694|\n",
    " |LLaMA 7B|\t112\t|1\t |128  |   2048|6,818|\n",
    " |LLaMA 7B|\t80\t|1\t |2048 |    128|2,244|\n",
    " |LLaMA 7B|\t48\t|1\t |2048 |   2048|2,740|\n",
    " |LLaMA 70B|1024|\t2|128  |    128|2,657|\n",
    " |LLaMA 70B|480\t|4\t |128  |2048   |1,486|\n",
    " |LLaMA 70B|96\t|2\t |2048 |128\t   |306|\n",
    " |LLaMA 70B|64  |\t2|2048 |2048   |547|\n",
    " |Falcon 180B|1024|\t4|\t128|128    |987|\n",
    " |Falcon 180B|1024|\t8|\t128|2048   |724|\n",
    " |Falcon 180B|\t64|\t4|2048 |128\t   |112|\n",
    " |Falcon 180B|\t64|\t4|2048 |2048   |264|\n",
    " \n",
    "\n",
    "Please click on the [Performance](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance.md) and [Benchmark](https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/python/README.md) links to see a detailed table on `throughput` and `low Latency` for H100, L40S (Ada) and A100 (Ampere)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27edc8c-7e21-4f04-8d8a-4d834b114f99",
   "metadata": {},
   "source": [
    "---\n",
    "## Acknowledgment\n",
    "\n",
    "This notebook is adapt from NVIDIA's [TensorRT-LLM Github repository](https://github.com/NVIDIA/TensorRT-LLM/tree/main)\n",
    "\n",
    "## References\n",
    "\n",
    "- https://nvidia.github.io/TensorRT-LLM/architecture.html\n",
    "- https://github.com/NVIDIA/TensorRT-LLM\n",
    "\n",
    "## Licensing\n",
    "Copyright © 2023 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c9683-d3d3-4111-913a-c64905b29a79",
   "metadata": {},
   "source": [
    " <div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"llama-chat-finetune.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "         <a>2</a>\n",
    "        <a href=\"triton-llama.ipynb\">3</a>\n",
    "        <a href=\"challenge.ipynb\">4</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"triton-llama.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
