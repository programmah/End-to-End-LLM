{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "835ad21a",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 75%; text-align: center;\">\n",
    "        <a>1</a>\n",
    "         <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "        <a href=\"triton-llama.ipynb\">3</a>\n",
    "        <a href=\"challenge.ipynb\">4</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 23%; text-align: right;\"><a href=\"trt-llama-chat.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a0283",
   "metadata": {},
   "source": [
    "# Finetuning Llama-2-7B with Custom Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27c48b",
   "metadata": {},
   "source": [
    "In this notebook we demostrate how to preprocess a dataset for text generation task and fine-tune with Llama-2-7b-chat model using 4-bit quantization via QLoRA allows efficient finetuning and Parameter-Efficient Fine-Tuning (PEFT) techniques that works by only updating a small subset of the model's parameters. The last section of the notebook describe the steps to run inference on the finetuned model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c11c44",
   "metadata": {},
   "source": [
    "## Overview of Llama 2\n",
    "\n",
    "Llama 2 is a family of pretrained and fine-tuned Large Language Models (LLMs) from [Meta](https://llama.meta.com/llama2) that consist of Llama-2 and Llama-2-chat. The The pretrained models ranging in scale from 7 billion to 70 billion parameters (7B, 13B, 70B) variants. The fine-tuned models (Llama 2-Chat) is optimized for conversational applications using Reinforcement Learning from Human Feedback ([RLHF](https://arxiv.org/abs/2305.18438)). Llama 2 is trained on a 2 trillion tokens and context length of 4K, therefore the model can grasp and generate extensive content. To improve inferecing scalabilty, Llama 2 adopt the [Grouped Query Attention (GQA)](https://arxiv.org/abs/2305.13245) that caches preivous token pairs to accelerates attention computation.\n",
    "\n",
    "<center><img src=\"images/llama2-chat-arc2.png\" width=\"900px\" height=\"900px\" alt-text=\"Arc\"/></center>\n",
    "<center><i>Source: </i> <a href=\"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\">Llama 2: Open Foundation and Fine-Tuned Chat Models </a> paper</center>\n",
    "    \n",
    "Our focus is to use a custom dataset and fine-tune using Llama 2-Chat 7 billion (Llama-2-7b-chat) variant. The variant was created using supervised fine-tuning and refined using RLHF technique by rejection sampling and [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347). Please run the cell below to download the `Llama-2-7b-chat` model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb544867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Llama-2-7b-chat model\n",
    "!python3 ../../source_code/Llama2/download-llama2-chat.py\n",
    "print(\"extracting files......\")\n",
    "!tar -xf ../../model/Llama-2-7b-chat.tar  -C ../../model\n",
    "print(\"files extraction done! removing tar file......\")\n",
    "!rm -rf ../../model/Llama-2-7b-chat.tar\n",
    "print(\"All done!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d839b03",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "Text Generation is the task of generating text that closely resembles human-written text. It is a process known as Causal Language Modelling where an AI model generate a coherent and meaningful text that imitate natural human communication. The task of text generation involves the use of algorithms and language models trained to learn patterns/long-term dependencies and contextual information to process input data and generate new output text based on given prompts. Popular large language models like  GPT (Generative Pre-trained Transformer), Mistral, and Llama 2, etc are used for the task. Use cases includes:\n",
    "\n",
    "- Instruction Models: adapt to follow instructions\n",
    "- Code Generation: trained on code from scratch to help the programmers in their repetitive coding tasks.\n",
    "- Stories Generation: receives an input text and proceed to create a story-like text based on the given text.\n",
    "You can read more about text generation task variants such as Completion Generation Models, Text-to-Text Generation Models, Text Generation from Image and Text, from [here](https://huggingface.co/tasks/text-generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed3143c",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We consider [openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) as our choice dataset for finetuning. The openassistant-guanaco dataset is a subset of the [Open Assistant Conversations](https://huggingface.co/datasets/OpenAssistant/oasst1/tree/main) dataset the contains only the highest-rated paths in the conversation tree, with a total of 9,85k training samples and 518 test samples. The OpenAssistant Conversations (OASST1) is a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. [source](https://huggingface.co/datasets/OpenAssistant/oasst1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a043a63b",
   "metadata": {},
   "source": [
    "### Dataset Structure\n",
    "\n",
    "Each row in the `openassistant-guanaco` jsonl dataset is a text dictionary that consists `human` instructions and `Assistant` that provide context to draw the response to the instruction. Within the `text` the `human` and `Assistant` fields are separated with three `###` delimeters that denote the start and end of positions.   \n",
    "\n",
    "```bash\n",
    "\n",
    "{ \"text\": \"### Human: Can you write a short introduction about the relevance of the term \\\"monopsony\\\" in economics? Please use examples related to...\n",
    "           ### Assistant: \\\"Monopsony\\\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term  is...   \n",
    "           ### Human: Now explain it to a dog\"\n",
    "}\n",
    "{ \"text\": \"### Human: \\u00bfCUales son las etapas del desarrollo y en qu\\u00e9 consisten seg\\u00fan Piaget?\n",
    "           ### Assistant: Jean Piaget fue un psic\\u00f3log suizo que propuso una teor\\u00eda sobre el desarrollo cognitivo...\n",
    "           ### Human: \\u00bfHay otras teor\\u00edas sobre las etapas del desarrollo que reafirmen o contradigan a la teor\\u00eda de Piaget?\"\n",
    "}\n",
    "{\"text\": \"### Human: Can you give me an example of a python script that opens an api point and serves a string?\n",
    "          ### Assistant: Sure! Here's an example Python script that uses the Flask web framework to create a simple API endpoint that serves a string:\\n\\n`         \\nfrom flask import Flask\\n\\napp = Flask(__name__)\\n\\n@app.route('/')\\ndef hello_world():\\n    return 'Hello, world!'\\n\\nif __name__ ==  \\n ...   \n",
    "          ### Human: What changes would you need to make to the code above to serve a JSON object instead of a string?\n",
    "          ### Assistant: To serve a JSON object instead of a string, you can modify the \\\"hello_world()\\\" function to return a JSON response using the  Flask \\\"jsonify\\\" function. Here's an example of how to modify the previous code to serve a JSON object:\\n\\n... \"\n",
    "}\n",
    "...\n",
    "\n",
    "```\n",
    "Please run the cell below to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c83560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "!python3 ../../source_code/Llama2/download-guanco-ds.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1579ca",
   "metadata": {},
   "source": [
    "Import all required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os \n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from langdetect import detect\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "import re\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3615d32",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d997b",
   "metadata": {},
   "source": [
    "Let's execute the `read_jsonl` function below to read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dbd377",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip3 install -U spacy[cuda114]\n",
    "#!pip3 install spacy-langdetect\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046cbadd",
   "metadata": {},
   "source": [
    "Let's perform the following steps in the cell below:\n",
    "- Set the path to the train and test jsonl files\n",
    "- Read both files using the `read_jsonl` function\n",
    "- Extract 5k samples from the training set\n",
    "- Display the samples to see the content and format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402426b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set path to jsonl files\n",
    "train_path = '../../data/openassistant_best_replies_train.jsonl'\n",
    "test_path = '../../data/openassistant_best_replies_eval.jsonl'\n",
    "\n",
    "# read the files\n",
    "raw_train_data = read_jsonl(train_path)\n",
    "raw_test_data = read_jsonl(test_path)\n",
    "\n",
    "# extract 5000 samples \n",
    "train_samples = raw_train_data[:5000]\n",
    "\n",
    "print(\"length of traning samples: \", len(train_samples))\n",
    "train_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e550a584",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```python\n",
    "\n",
    "length of traning samples:  5000\n",
    "[{'text': '### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as ..'},\n",
    " ...\n",
    " {'text': '### Human: ¬øCUales son las etapas del desarrollo y en qu√© consisten seg√∫n Piaget?### Assistant: Jean Piaget fue un psic√≥logo suizo que propuso una teor√≠a sobre el desarrollo cognitivo humano que consta de cuatro etapas:\\n\\nEtapa sensoriomotora (0-2 a√±os): Durante esta etapa, el ni√±o aprende a trav√©s de sus sentidos y movimientos. Descubre que sus acciones pueden tener un impacto en el entorno y comienza a formarse una idea b√°sica de objetividad y continuidad.\\n\\nEtapa preoperatoria (2-7 a√±os): En esta etapa, el ni√±o comienza a desarrollar un pensamiento simb√≥lico y a comprender que las cosas pueden representar a otras cosas. Tambi√©n comienzan a desarrollar un pensamiento l√≥gico y a comprender conceptos como la causa y el efecto.\\n\\nEtapa de ...?'},\n",
    " ...\n",
    " {'text': '### Human: Schreibe einen kurze und pr√§zise Konstruktionsbeschreibung zu einem Dreieck ABC mit c=6\\xa0cm, h_c=5\\xa0cm und Œ≥=40¬∞. Œ≥ ist hierbei der von Seite c gegen√ºberliegende Winkel.### Assistant: Dreieck ABC ist ein rechtwinkliges Dreieck mit der Seitenl√§nge c=6 cm als Hypotenuse. Die H√∂he h_c von c betr√§gt 5 cm und der Winkel Œ≥ von c gegen√ºberliegend betr√§gt 40¬∞.### Human: Vielen Dank, das hat mir sehr weitergeholfen.'},\n",
    " {'text': '### Human: –ù–∞–ø–∏—à–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏–≥—Ä–µ Hytale### Assistant: Hytale - —ç—Ç–æ –ø—Ä–µ–¥—Å—Ç–æ—è—â–∞—è –∏–≥—Ä–∞-–ø–µ—Å–æ—á–Ω–∏—Ü–∞, –ø—Ä–∏–¥—É–º–∞–Ω–Ω–∞—è Hypixel Studios. –û–±—ä–µ–¥–∏–Ω—è—è –≤ —Å–µ–±–µ –¥–∏–∞–ø–∞–∑–æ–Ω –∏–≥—Ä—ã-–ø–µ—Å–æ—á–Ω–∏—Ü—ã —Å –≥–ª—É–±–∏–Ω–æ–π —Ä–æ–ª–µ–≤–æ–π –∏–≥—Ä—ã, Hytale –ø–æ–≥—Ä—É–∂–∞–µ—Ç –∏–≥—Ä–æ–∫–æ–≤ –≤ –∂–∏–≤–æ–π –∫–æ—Å–º–æ—Å, –ø–æ–ª–Ω—ã–π —ç—Å–∫–∞–ø–∞–¥ –∏ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞.\\n\\n–í Hytale –≥–µ–π–º–µ—Ä—ã –º–æ–≥—É—Ç –ø—É—Ç–µ—à–µ—Å—Ç–≤–æ–≤–∞—Ç—å –ø–æ –ø—Ä–æ—Å—Ç–æ—Ä–Ω–æ–º—É, –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É –º–∏—Ä—É –∏ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —É—á–∞—Å—Ç–∏–µ –≤ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö. –û—Ç –≤–æ–∑–≤–µ–¥–µ–Ω–∏—è –±–∞–∑ –∏ —Å–æ–∑–¥–∞–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –¥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—Ä—Ç –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏–π –∏ —Å—Ä–∞–∂–µ–Ω–∏–π —Å –º–æ–Ω—Å—Ç—Ä–∞–º–∏ –∏–ª–∏ –¥—Ä—É–≥–∏–º–∏ –∏–≥—Ä–æ–∫–∞–º–∏ - –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Hytale –±–µ–∑–≥—Ä–∞–Ω–∏—á–Ω—ã. –ò–≥—Ä–æ–∫–∏ —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–≤–æ–∏ ..'},\n",
    " {'text': '### Human: ÁßÅ„ÅØÁå´„Å°„ÇÉ„Çì„ÅåÂ•Ω„Åç„Å™„Çì„Åß„Åô„Åë„Å©\\nË≤¥Êñπ„ÇÇÁå´„Å°„ÇÉ„Çì„ÅåÂ•Ω„Åç„Åß„Åô„Åã?### Assistant: Áå´„Å°„ÇÉ„Çì„ÅåÂ•Ω„Åç„Å™„Çì„Åß„Åô„Å≠„ÄÇ\\nÁßÅ„ÇÇÁå´„Å°„ÇÉ„Çì„ÅåÂ•Ω„Åç„Åß„Åô„ÇàÔºÅ\\nÁå´„Å°„ÇÉ„Çì„ÅÆ„Å©„Çì„Å™„Å®„Åì„Çç„ÅåÂ•Ω„Åç„Åß„Åô„ÅãÔºü### Human: Áå´„Å°„ÇÉ„Çì„ÅÆÂÖ®„Å¶„ÅåÂ•Ω„Åç„Åß„ÅôÔºÅÁå´„Å°„ÇÉ„Çì„ÅØÂ§©‰Ωø„Åß„ÅôÔºÅÔºÅ### Assistant: „Çè„Åã„Çä„Åæ„Åô„Çà„ÄÅ„Åù„ÅÆÊ∞óÊåÅ„Å°ÔºÅ\\nÁå´„Å°„ÇÉ„Çì„ÅÆÊÑõ„Åè„Çã„Åó„ÅÑÂßø„ÅØ„ÄÅÂ§©‰Ωø„Åø„Åü„ÅÑ„Å´ÂèØÊÑõ„ÅÑ„Åß„Åô„Çà„Å≠ÔºÅ\\n„ÅÇ„Å™„Åü„ÅÆ„ÅäÂÆ∂„Å´„ÄÅÁå´„Å°„ÇÉ„Çì„ÅØ„ÅÑ„Çã„Çì„Åß„Åô„ÅãÔºü### Human: ÂãøË´ñ„Åß„ÅôÔºÅ„Å®„Å£„Å¶„ÇÇÂèØÊÑõ„Åè„Å¶„ÄÅÊØéÊó•„ÅåÂπ∏„Åõ„Åß„ÅôÔºÅ'},\n",
    "\n",
    " {'text': \"### Human: Quins sin√≤nims amb altres dialectes d'aquesta llengua t√© nen o nena?### Assistant: Al¬∑lot o al¬∑lota, vailet o vaileta, manyac o manyaga, nin o nina, xiquet o xiqueta, xic o xica, marrec, miny√≥ o minyona.\"},\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2494870d",
   "metadata": {},
   "source": [
    "We can see from the content displayed that the file contain text written in different languages. For demostration purpose, we want to consider only english text for training and prompting. Therefore, the train and test sets will be processed to filter out non-English text samples using a `detect` feature from `spacy-langdetect` library. The English-only texts (`1778 training samples`) are saved as the new training set. The same process is applied to the test samples. Run the three two cells below to execute the filter process.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter non-Emglish word\n",
    "def remove_nonEnglish_rows(ds):\n",
    "    new_ds = []\n",
    "    for row in (ds):\n",
    "        if detect(str(row)) == 'en':\n",
    "            new_ds.append(row)\n",
    "    return new_ds\n",
    "        \n",
    "# save English text samples\n",
    "def save_jsonl(ds,filename):\n",
    "    with open(f\"../../data/filtered/{filename}.jsonl\", \"w\") as write_file:\n",
    "            json.dump(ds, write_file, indent=4)\n",
    "            print(\"dataset saved in jsonl format ....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160afdcd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calling filter function\n",
    "filter_train_samples = remove_nonEnglish_rows(train_samples)\n",
    "filter_test_samples = remove_nonEnglish_rows(raw_test_data )\n",
    "\n",
    "print(\"len of training samples: \",len(filter_train_samples))\n",
    "filter_train_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4905d",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```python\n",
    "len of training samples:  1775\n",
    "[{'text': '### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as ...'},\n",
    " {'text': '### Human: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?### Assistant: Sure! Let\\'s say you want to build a model which can distinguish between images of cats and dogs. You gather your dataset, consisting of many cat and dog pictures. Then you put them through a neural net of your choice, which produces some representation for each image, a sequence of numbers like [0.123, 0.045, 0.334, ...]. The problem is, if your model is unfamiliar with cat and dog images, these representations will be quite random. At one time a cat and ...'},\n",
    " {'text': \"### Human: I want to start doing astrophotography as a hobby, any suggestions what could i do?### Assistant: Getting started in astrophotography can seem daunting, but with some patience and practice, you can become a master of the craft. To begin, you'll need a good camera and lens, a tripod, and a dark sky location free of light pollution. You will also need to learn about the basics of astrophotography, such as what camera settings to use, how to capture star trails, and the best techniques for tracking celestial objects. You can also purchase or rent different types of telescopes, depending on what types of objects you want to capture. Additionally, it's important to keep up with the latest astrophotography news and trends. Once you have the necessary ...\"},\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a51c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file names  \n",
    "save_train_filename = 'train'\n",
    "save_test_filename = 'test'\n",
    "\n",
    "# save file\n",
    "save_jsonl(filter_train_samples, save_train_filename)\n",
    "save_jsonl(filter_test_samples, save_test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e0bb44",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Transformation\n",
    "\n",
    "In this section, we want to format our dataset to the Llama2 acceptable template for finetuning:\n",
    "\n",
    "```python\n",
    "\n",
    "<s>[INST] {human text} [/INST] {assistant/context} </s>\n",
    "<s>[INST]{human text} [/INST] </s>\n",
    "\n",
    "```\n",
    "- **Human text**: it denotes instruction issued by human to the model. The human text is enclosed within an instruction tag `[INST] [/INST]` \n",
    "- **Assistance**: represents the context that will assist the model to draw out response to the instruction issued by human. The assistant text is nested to the human text within a segment tag `<s>  </s>`.\n",
    "\n",
    "One or more segments can exist within a training sample text. A segment can consist of both human instruction text and assistant context or human instuction text only. \n",
    "\n",
    "<img src=\"images/template.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998a5dbf",
   "metadata": {},
   "source": [
    "Next, we load the filtered dataset by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf194dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('../../data/filtered')\n",
    "#DS_test = dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae973f",
   "metadata": {},
   "source": [
    "Execute the function to transform the filtered dataset to the format explained above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91131070",
   "metadata": {},
   "outputs": [],
   "source": [
    "### credit: https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k ### \n",
    "\n",
    "# Define a function to transform the data\n",
    "def transform_to_template(example):\n",
    "    conversation_text = example['text']\n",
    "    segments = conversation_text.split('###')\n",
    "\n",
    "    reformatted_segments = []\n",
    "\n",
    "    # Iterate over pairs of segments\n",
    "    for i in range(1, len(segments) - 1, 2):\n",
    "        human_text = segments[i].strip().replace('Human:', '').strip()\n",
    "\n",
    "        # Check if there is a corresponding assistant segment before processing\n",
    "        if i + 1 < len(segments):\n",
    "            assistant_text = segments[i+1].strip().replace('Assistant:', '').strip()\n",
    "\n",
    "            # Apply the new template\n",
    "            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] {assistant_text} </s>')\n",
    "        else:\n",
    "            # Handle the case where there is no corresponding assistant segment\n",
    "            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] </s>')\n",
    "\n",
    "    return {'text': ''.join(reformatted_segments)}\n",
    "\n",
    "\n",
    "# Apply the transformation\n",
    "template_dataset = dataset.map(transform_to_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f65371",
   "metadata": {},
   "source": [
    "Let's display a sample to inspect if our training set is in the right format as shown in the screenshot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71536d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dataset['train'][2:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e14b1",
   "metadata": {},
   "source": [
    "Save the preprocessed dataset to the directory `../data/ds_preprocess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dataset.save_to_disk('../../data/ds_preprocess')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c76a3",
   "metadata": {},
   "source": [
    "## Fine-tuning Llama 2\n",
    "\n",
    "As ealier mentioned in the notebook, our choice model to finetune is the `Llama-2-7b-chat`. Below is the list of walkthrough steps to complete the task.\n",
    "\n",
    "- Convert the `Llama-2-7b-chat` to Hugging Face transformer format\n",
    "- Set the paths to the model and laod the dataset\n",
    "- Load the model tokenizer\n",
    "- Set the training parameter\n",
    "- Configure the Parameter Efficient Fine Tuning with LoRA\n",
    "- Apply 4-bits quantization\n",
    "- Setup the trainer to start the fine tuning process \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c32e57",
   "metadata": {},
   "source": [
    "### Convert model to Hugging Face Transformers Format\n",
    "We will convert our model checkpoint to Hugging Face transformer format. The benefit are as follows:  \n",
    "\n",
    "- Delivers a convenient way to finetune Llama2 model with limited GPU computing resource (like laptops, workstations, or Google Colab) through the use of some technique that a transformers compactible \n",
    "- Ability to immediately use a model on a given input text using transformer Pipelines. \n",
    "- The use of transformers `Pipelines` group together the pretrained model with the preprocessing that was used during that training to enable quick inferencing.\n",
    "\n",
    "\n",
    "To convert our model checkpoint, we use the `convert_llama_weights_to_hf.py` script located on [GitHub](https://github.com/cedrickchee/transformers-llama/tree/llama_push/src/transformers/models/llama). \n",
    "\n",
    "- `--input_dir`: denotes the directory to the llama model to the converted\n",
    "- `--model_size`: represents the llama model parameter size\n",
    "- `--out_dir`: the directory to save the converted model\n",
    "\n",
    "<img src=\"images/llama-hf.png\" height=\"550px\" width=\"900px\" />\n",
    "\n",
    "If you already have the Hugging Face format of the model `Llama-2-7b-chat-hf`, you can skip running the cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066766f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../../source_code/Llama2/llama/convert_llama_weights_to_hf.py \\\n",
    "--input_dir ../../model/Llama-2-7b-chat --model_size 7B --output_dir ../../model/Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd677497",
   "metadata": {},
   "source": [
    "**Exepected Output:**\n",
    "```python\n",
    "...\n",
    "https://github.com/huggingface/transformers/pull/24565\n",
    "Fetching all parameters from the checkpoint at ../model/Llama-2-7b-chat.\n",
    "Loading the checkpoint in a Llama model.\n",
    "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:06<00:00,  4.95it/s]\n",
    "Saving in the Transformers format.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03118f57",
   "metadata": {},
   "source": [
    "Now, we can initialize the path to our transformer format model and load the transformed/preprocessed training dataset from the directory where it was saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initailize path to the base model \n",
    "base_model = \"../../model/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# set the path to the dataset template\n",
    "data_path = \"../../data/ds_preprocess/train\"\n",
    "\n",
    "# set the path to the dataset template\n",
    "eval_path = \"../../data/ds_preprocess/test\"\n",
    "\n",
    "# load the transformed dataset\n",
    "dataset = load_from_disk(data_path)\n",
    "eval_dataset = load_from_disk(eval_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7154f6c",
   "metadata": {},
   "source": [
    "### Loading tokenizer\n",
    "\n",
    "Tokenization is the process of breaking up a text into sentences or words. Each word or sentence in a text is considered as a token. Tokenization allows a detailed analysis of text data when it is broken into smaller units. The [LLaMA tokenizer](https://huggingface.co/docs/transformers/en/model_doc/llama2) is a byte-pair-encoding (BPE) model based on [sentencepiece](https://aclanthology.org/P16-1162/), an unsupervised text tokenizer and detokenizer for Neural Network-based text generation systems that predetermined the vocabulary size prior to the neural model training.\n",
    "\n",
    "In the cell below, we load tokenizer from our base model directory and set parameters:\n",
    "\n",
    "- **pad_token**: a special token used to make arrays of tokens the same size for batching purpose. \n",
    "- **padding_side**: side to pad\n",
    "\n",
    "A comprehensive list of Llama tokenizer parameters can be found [here](https://huggingface.co/docs/transformers/en/model_doc/llama2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856da78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_from_disk\n",
    "#dataset = load_dataset(data_path, split=\"train\")\n",
    "#dataset = load_dataset(data_path)\n",
    "#dataset = load_from_disk(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb4189",
   "metadata": {},
   "source": [
    "### Set Training Parameters\n",
    "\n",
    "Training parameters are customize using the `TrainingArguments` class. The class provides an API that offers a wide range of options to customize and optimize the training process. Please find a comprehensive description of the parameters [here](https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#transformers.TrainingArguments). You can further modify the values of the parameters in the next cell after the complet finetune process and rerun the cells to see how it impact on the training outcome.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"../../model/results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    group_by_length=True,\n",
    "    save_steps=50,\n",
    "    logging_steps=50, \n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95942f99",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure PEFT With LoRA\n",
    "\n",
    "Fine-tuning large language pretrained models is computational costly. Our main goal is to accelerate our finetuning process with minimal memory consumption. A state-of-the-art method to achieve that is using [Parameter-Efficient Fine-Tuning (PEFT)](https://github.com/huggingface/peft/tree/main) approach. [PEFT](https://arxiv.org/abs/2305.16742) allows fine-tuning a small number of (extra) model parameters instead of all the model's parameters, and this significantly decreases the computational and storage costs. One of the ways to implement PEFT is to adopt the Low-Rank Adaptation (LoRA) technique. Lora makes fine-tuning more efficient by greatly reducing the number of trainable parameters for downstream tasks. It does this by freezing the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture. According to the [authors of LoRA](https://arxiv.org/abs/2106.09685), Aside reducing number of trainable parameters by 10k times, it also reduce the GPU consumption by 3x, thus, delivering high throughput with no inference latentcy. For quick background on LoRA, please follow this [link](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora). \n",
    "\n",
    "<center><img src=\"images/lora-arch.png\" height=\"500px\" width=\"900px\"  /></center>\n",
    "<center>  LoRA reparametrization and Weight merging. <a href=\"https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\"> View source</a> </center>\n",
    "\n",
    "LoRA techniques is applied through `LoraConfig` which provides PEFT parameters that control how the technique is applied to the base model. Description of the parameter used in the cell below is given as follows:\n",
    "\n",
    "- **lora_alpha**: LoRA scaling factor\n",
    "- **lora_dropout**: The dropout probability for Lora layers.\n",
    "- **r**: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "- **bias**: Specifies if the bias parameters should be trained. Can be 'none', 'all' or 'lora_only'.\n",
    "- **task_type**: Possible task types which include `CAUSAL_LM`, `FEATURE_EXTRACTION`, `QUESTION_ANS`, `SEQ_2_SEQ_LM`, and `SEQ_CLS and TOKEN_CLS`.   \n",
    "\n",
    "Because the task we want to perform is text generation, we have set the task_type to Causal language model `(CAUSAL_LM)` which is frequently used for text generation taks. Please run the cell below to setup the LoRA config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a2e65",
   "metadata": {},
   "source": [
    "### 4-bit quantization configuration\n",
    "\n",
    "Model quantization is a popular deep learning optimization method in which model data‚Äîboth network parameters and activations‚Äîare converted from a floating-point representation to a lower-precision representation, typically using 8-bit integers. Quantization represents data with fewer bits, making it a useful technique for reducing memory-usage and accelerating inference especially when it comes to large language models (LLMs). It can be combined with PEFT methods to make it easier to train and load LLMs for inference.\n",
    "\n",
    "<center><img src=\"images/quantization.png\" height=\"400px\" width=\"700px\" /></center>\n",
    "<center> <a href=\"https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/\" > source: Using Quantization Aware Training with NVIDIA TensorRT</a></center>\n",
    "\n",
    "Several ways and algorithms to quantize a model including can be found [here](https://huggingface.co/docs/peft/main/en/developer_guides/quantization). A library to easily implement quantization and integrate with transformers is the `bitsandbytes` library. The library provides config parameters to quantize a model to 8 or 4-bits using the `BitsAndBytesConfig` class. The 4 bits parameters used in the cell below are describe as follows:\n",
    "\n",
    "- **load_in_4bit**: set `True` to quantize the model to 4-bits when you load it\n",
    "- **bnb_4bit_quant_type**: set to `\"nf4\"` to use a special 4-bit data type for weights initialized from a normal distribution\n",
    "- **bnb_4bit_use_double_quant**: set `True` to use a nested quantization scheme to quantize the already quantized weights\n",
    "- **bnb_4bit_compute_dtype**: set to `torch.float16` or `torch.bfloat16` to use bfloat16 for faster computation \n",
    "\n",
    "Run the cell below to setup the 4-bit quantization for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cb92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a62d90",
   "metadata": {},
   "source": [
    "### Loading Base Model\n",
    "\n",
    "The next step is to load our base model `(Llama-2-7b-chat-hf)` with causal language model class used for text generation task. We do this by passing the base model , quantization config, and GPU device id to the `AutoModelForCausalLM` object as shown in the cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a83e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d06b3",
   "metadata": {},
   "source": [
    "### Set the Trainer Parameter\n",
    "\n",
    "To instiate our model tainer, we create a trainer object from [Supervised fine-tuning (SFT)](https://huggingface.co/docs/trl/en/sft_trainer). SFT is part of integrated transformer [Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/en/index) tools used to train transformer language models with Reinforcement Learning. Others include [Reward Modeling step (RM)](https://huggingface.co/docs/trl/en/reward_trainer) and  Proximal [Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347). In our SFT trainer object we set our model, training dataset, PEFT config  object, model tokenizer, training argument parameter, and also specify the field (`text`) to use within our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aaafea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_params,\n",
    "    args=training_params,\n",
    "    max_seq_length=None,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f95b3",
   "metadata": {},
   "source": [
    "Run the cell below to train the SFT trainer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c29ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8008587a",
   "metadata": {},
   "source": [
    "Save the finetuning model and tokenizer in the directory  `../model/Llama-2-7b-chat-hf-finetune`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd701b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "new_model = \"../../model/Llama-2-7b-chat-hf-finetune\"\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8e21e",
   "metadata": {},
   "source": [
    "If you are running this notebook on your personal laptop, worskstaion, you can check training loss by loading the Tensorbord through a specific port using the code snippet below.\n",
    "\n",
    "<center><img src=\"images/training_log.png\" height=\"700px\" width=\"800px\"/></center>\n",
    "\n",
    "```python\n",
    "from tensorboard import notebook\n",
    "training_log_path = \"../model/results/runs\"\n",
    "notebook.start(\"--logdir {} --port 6060\".format(training_log_path))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b219ed",
   "metadata": {},
   "source": [
    "## Inferencing\n",
    "\n",
    "Now that we completed our finetuning process, we can run a quick inference to test how our new model is performing. To do that, we will create the following:\n",
    "\n",
    "- a transformer pipeline with four parameter inputs: `model`, `tokenizer`, `max_length`, and `task`.  \n",
    "- format prompt as: `f\"<s>[INST] {prompt} [/INST]\"`\n",
    "- pass the prompt into the pipeline object and get result.\n",
    "\n",
    "```python\n",
    " \n",
    "inf_pipeline = pipeline(model=model, tokenizer=tokenizer, max_length=200, task=\"text-generation\")\n",
    "prompt = inf_pipeline(f\"<s>[INST] {prompt} [/INST]\")\n",
    "result = inf_pipeline(prompt)\n",
    "print(result[0]['generated_text'])\n",
    "```\n",
    "You can modify the `max_length` to decide the length of text generated by the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "def run_inference(prompt):\n",
    "    inf_pipeline = pipeline(model=model, tokenizer=tokenizer, max_length=200, task=\"text-generation\")\n",
    "    prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "    result = inf_pipeline(prompt)\n",
    "    print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a654ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"explain what is astrophotography?\"\n",
    "run_inference(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534653a3",
   "metadata": {},
   "source": [
    "**Likely output:**\n",
    "\n",
    "```bash\n",
    "<s>[INST] explain what is astrophotography? [/INST] Astrophotography is the branch of photography that deals with the photographing of celestial objects such as stars, planets, galaxies, and other astronomical phenomena. Astrophotographers use specialized cameras and telescopes to capture images of these objects, often in low light conditions. Astrophotography requires a great deal of patience, skill, and knowledge, as photographers must be able to accurately track the movement of celestial objects and compensate for the effects of light pollution and atmospheric distortion. Astrophotography has become increasingly popular in recent years, with many amateur astronomers and professional photographers engaging in this hobby and art form. \n",
    "\n",
    "Astrophotography can be done using a variety of techniques, including long exposure times, tracking mounts, and using narrow-band filters to\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f37554",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"can you explain further?\"\n",
    "run_inference(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1213dec0",
   "metadata": {},
   "source": [
    "**Likely output:**\n",
    "\n",
    "```bash\n",
    "<s>[INST] can you explain further? [/INST] Sure, I'd be happy to explain further! Could you please provide more context or clarify what you're asking about? üòÉ üëç üí° üñä üìù üìÖ üï∞ üï∞Ô∏è üï≥Ô∏è üï∑Ô∏è üï∏Ô∏è üï∑Ô∏è üï∏Ô∏è üï≥Ô∏è üï∑Ô∏è üï∏Ô∏è üï≥Ô∏è üï∑Ô∏è üï∏Ô∏è üï∞Ô∏è üï∞Ô∏è üï≥Ô∏è üï∑Ô∏è üï∏Ô∏è üï≥Ô∏è üï∑Ô∏è üï∏Ô∏è üï∞Ô∏è\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88d19a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"I want you to explain further on astrophotography\"\n",
    "run_inference(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bea959",
   "metadata": {},
   "source": [
    "**Likely output:**\n",
    "\n",
    "```bash\n",
    "\n",
    "<s>[INST] I want you to explain futher on astrophotography [/INST] Sure, I d be happy to explain further on astrophotography. Astrophotography is the process of capturing images of celestial objects such as stars, planets, galaxies, and other astronomical phenomena. It involves using specialized cameras and techniques to capture high-quality images of these objects in the night sky.\n",
    "\n",
    "There are several key components to astrophotography:\n",
    "\n",
    "    Camera: Astrophotography cameras are designed specifically for capturing images of celestial objects. They typically have large sensors, fast lenses, and specialized features such as built-in equatorial mounts, motorized tracking systems, and cooling systems to reduce noise and improve image quality.\n",
    "\n",
    "    Telescope: A telescope is used to gather light from the celestial object being photographed. There are several types of\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4729fe7d",
   "metadata": {},
   "source": [
    "### Reload model in FP16 and merge with LoRA weights\n",
    "\n",
    "To have our model as single entity for ease of use and widen task coverage, we reload the model in fp16 mode and merge with the LoRA weights using `model.merge_and_unload()`. The tokenizer is also reload, pad, and saved along with the merged model in same directory, `../model/Llama-2-7b-chat-hf-merged`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a10fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "load_base_model = AutoModelForCausalLM.from_pretrained( base_model, torch_dtype=torch.float16, low_cpu_mem_usage=True, return_dict=True, device_map={\"\": 0})\n",
    "\n",
    "model = PeftModel.from_pretrained(load_base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f221220",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../../model/Llama-2-7b-chat-hf-merged\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"../../model/Llama-2-7b-chat-hf-merged\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ba926",
   "metadata": {},
   "source": [
    "**expected output**:\n",
    "```python\n",
    "('../model/Llama-2-7b-chat-hf-merged/tokenizer_config.json',\n",
    " '../model/Llama-2-7b-chat-hf-merged/special_tokens_map.json',\n",
    " '../model/Llama-2-7b-chat-hf-merged/tokenizer.model',\n",
    " '../model/Llama-2-7b-chat-hf-merged/added_tokens.json',\n",
    " '../model/Llama-2-7b-chat-hf-merged/tokenizer.json')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4853c72",
   "metadata": {},
   "source": [
    "---\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70673aa",
   "metadata": {},
   "source": [
    "- https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\n",
    "- https://llama.meta.com/llama2\n",
    "- https://huggingface.co/tasks/text-generation\n",
    "- https://arxiv.org/abs/2304.07327\n",
    "- https://huggingface.co/datasets/timdettmers/openassistant-guanaco\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/llama2\n",
    "- https://huggingface.co/docs/peft/main/en/developer_guides/quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85acab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Licensing\n",
    "\n",
    "Copyright ¬© 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6fc7b",
   "metadata": {},
   "source": [
    " <div>\n",
    "    <span style=\"float: left; width: 75%; text-align: center;\">\n",
    "         <a>1</a>\n",
    "         <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "        <a href=\"triton-llama.ipynb\">3</a>\n",
    "        <a href=\"challenge.ipynb\">4</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 23%; text-align: right;\"><a href=\"trt-llama-chat.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../../Start_Here.ipynb\">Home Page</a> </center> </p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
