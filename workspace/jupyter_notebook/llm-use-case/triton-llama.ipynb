{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb26416",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"trt-llama-chat.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "        <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "        <a>3</a>\n",
    "        <a href=\"challenge.ipynb\">4</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"challenge.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e7bb9-4bc4-4e0c-bc99-bbd0408899c5",
   "metadata": {},
   "source": [
    "# Deploying Finetune Model using Triton Inference Server \n",
    "--- \n",
    "\n",
    "In this notebook our focus would be the use of TensorRT-LLM Backend to deploy tensorrt engine built in the previous notebook. The goal of TensorRT-LLM Backend is to let you serve [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) models with Triton Inference Server. You can learn more about Triton backends in the [backend repo](https://github.com/triton-inference-server/backend)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191517a-8582-4c90-926b-768d2faf2428",
   "metadata": {},
   "source": [
    "\n",
    "## Using the TensorRT-LLM Backend\n",
    "\n",
    "To use the TensorRT-LLM Backend, follow the steps below:\n",
    "\n",
    "- Clone the TensorRT-LLM Backend repo ([tensorrtllm_backend](https://github.com/triton-inference-server/tensorrtllm_backend)), and this we have done within the container for running the lab.\n",
    "   <img src=\"images/trt-clone.png\" />\n",
    "- Navigate into the create the model repository `triton_model_repo`\n",
    "   <img src=\"images/trt-model-repo.png\" />\n",
    "- Copy all files in `all_models/inflight_batcher_llm` directory into the `triton_model_repo`\n",
    "   <img src=\"images/trt-copy-folders.png\" />\n",
    "\n",
    "- There are four model directories within the `all_models/inflight_batcher_llm`. The directories include:\n",
    "    - **preprocessing**: This model is used for tokenizing, meaning the conversion from prompts(string) to input_ids(list of ints).\n",
    "    - **tensorrt_llm**: This model is a wrapper of your TensorRT-LLM model and is used for inferencing\n",
    "    - **postprocessing**: This model is used for de-tokenizing, meaning the conversion from output_ids(list of ints) to outputs(string).\n",
    "    - **ensemble**: This model is used to chain the three models above together as: `preprocessing -> tensorrt_llm -> postprocessing`\n",
    "\n",
    "To learn more about ensemble model, please see\n",
    "[here](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#ensemble-models).\n",
    "\n",
    "- Copy the model tensorrt engine from `../../model/trt_engines/fp16/1-gpu` into the `triton_model_repo/tensorrt_llm/1` directory\n",
    "   <img src=\"images/trt-copy-engine.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88c8d1-28a0-4077-9c11-7d5d5ffbc5c5",
   "metadata": {},
   "source": [
    "Now, let's execute the above steps. But before that, we have to copy the `tensorrtllm_backend` repo from within our container to the `source_code` directory for ease of access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb883fb-6b7d-4889-823f-904fc1ab1e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy tensorrtllm_backend to source_code\n",
    "!cp -r /workspace/tensorrtllm_backend /workspace/app/source_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df948700-0cbc-4abf-97bf-2a006c22267b",
   "metadata": {},
   "source": [
    "Next, we navigate into the `tensorrtllm_backend` and execute all the steps mentioned above for `FP16 tensorrt engine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442ceca-28bc-459a-b4ef-1e186e08faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create the model repository that will be used by the Triton server\n",
    "cd /workspace/app/source_code/tensorrtllm_backend/\n",
    "mkdir triton_model_repo\n",
    "\n",
    "# Copy the example models to the model repository\n",
    "cp -r all_models/inflight_batcher_llm/* triton_model_repo/\n",
    "\n",
    "# Copy the TRT engine to triton_model_repo/tensorrt_llm/1/\n",
    "cp  /workspace/app/model/trt_engines/fp16/1-gpu/* triton_model_repo/tensorrt_llm/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872f9e7-0a08-4268-8b75-15421cce7e15",
   "metadata": {},
   "source": [
    "### Modify the model configuration\n",
    "\n",
    "The following table shows the fields that need to be modified before deployment:\n",
    "\n",
    "<div><center>\n",
    "<img src=\"images/ensemble.png\" width=\"1000\"/>\n",
    "</center></div>\n",
    "\n",
    "\n",
    "**Kindly change the key-value pair using the below table to set the correct configuration, and do not forget to save it.**\n",
    "\n",
    "- a) Kindly open the **Pre-processing config file** and make the changes listed in the table below *[triton_model_repo/preprocessing/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/preprocessing/config.pbtxt)*\n",
    "\n",
    "| Line # | Name | Description | \n",
    "| :----------------------:| :----------------------: | :-----------------------------: | \n",
    "| 83 | `tokenizer_dir` | The path to the tokenizer for the model. Please set the path to **`/workspace/app/model/Llama-2-7b-chat-hf-merged`**|\n",
    "| 90 | `tokenizer_type` | The type of the tokenizer for the model, `t5`, `auto` and `llama` are supported. Please set the type to **`llama`** |\n",
    "\n",
    "<center><img src=\"images/trt-preconfig.png\"  alt-text=\"server\"/></center>\n",
    "\n",
    "---\n",
    "\n",
    "- b) Kindly open the **tensorrt llm config file** and make the changes listed in the table below *[triton_model_repo/tensorrt_llm/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm/config.pbtxt)*\n",
    "\n",
    "| Line # | Name | Description\n",
    "| :----------------------: | :----------------------: | :-----------------------------: |\n",
    "| 32|`decoupled` | Controls streaming. Decoupled mode must be set to `True` if using the streaming option from the client. Please set Decoupled to **`False`** |\n",
    "| 170|`gpt_model_type` | Set to `inflight_fused_batching` when enabling in-flight batching support. To disable in-flight batching, set to `V1` , Please set as **`V1`**|\n",
    "| 176|`gpt_model_path` | Path to the TensorRT-LLM engines for deployment. Please set the path to **`/workspace/app/source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm/1`** |\n",
    "\n",
    "<center><img src=\"images/trt-llmconfig.png\"  alt-text=\"server\"/></center>\n",
    "\n",
    "---\n",
    "- c) Kindly open the **Post-processing config file** and make the changes listed in the table below *[triton_model_repo/postprocessing/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/postprocessing/config.pbtxt)*\n",
    "\n",
    "| Line # | Name | Description\n",
    "| :----------------------:| :----------------------: | :-----------------------------: |\n",
    "| 48 | `tokenizer_dir` | The path to the tokenizer for the model. Please set the path to **`/workspace/app/model/Llama-2-7b-chat-hf-merged`** |\n",
    "| 55 | `tokenizer_type` | The type of the tokenizer for the model, `t5`, `auto` and `llama` are supported. Please set the type to **`llama`** |\n",
    "\n",
    "<center><img src=\"images/postconfig.png\"  alt-text=\"postconfig\"/></center>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adad65b-a9e2-4c6d-96e5-b7cc3d2c151a",
   "metadata": {},
   "source": [
    "### Launch Triton server\n",
    "\n",
    "We can launch the Triton server with the following command:\n",
    "\n",
    "- Press `Crtl+Shift+L` and open a new terminal\n",
    "  <center><img src=\"images/terminal.png\"  alt-text=\"terminal\"/></center>\n",
    "- On the terminal, navigate to the launch script folder by running this command: `cd /workspace/app/source_code/tensorrtllm_backend`\n",
    "- Start the Triton Server with this command: `python3 scripts/launch_triton_server.py --tritonserver \"/opt/tritonserver/bin/tritonserver --http-port $HTTP_PORT --allow-grpc False --allow-metrics False\"  --world_size=1  --model_repo=/workspace/app/source_code/tensorrtllm_backend/triton_model_repo`\n",
    "\n",
    "\n",
    "It will take a few minutes to run and when successfully deployed, the server produces logs similar to the screenshot below.\n",
    "\n",
    "<center><img src=\"images/triton-server.png\"  alt-text=\"server\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec076e-f1e9-4b22-b5ef-d750cf2bd136",
   "metadata": {},
   "source": [
    "## Query the server with the Triton-generated endpoint\n",
    "\n",
    "You can query the server using Triton's\n",
    "[generate endpoint](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_generate.md)\n",
    "with a curl command based on the following general format within your client\n",
    "environment/container:\n",
    "\n",
    "```bash\n",
    "curl -X POST localhost:$HTTP_PORT/v2/models/${MODEL_NAME}/generate -d '{\"{PARAM1_KEY}\": \"{PARAM1_VALUE}\", ... }'\n",
    "```\n",
    "\n",
    "In the case of the models used in this example, you can replace MODEL_NAME with `ensemble`. Examining the\n",
    "ensemble model's config.pbtxt file, you can see that 4 parameters are required to generate a response\n",
    "for this model:\n",
    "\n",
    "- \"text_input\": Input text to generate a response from\n",
    "- \"max_tokens\": The number of requested output tokens\n",
    "- \"bad_words\": A list of bad words (can be empty)\n",
    "- \"stop_words\": A list of stop words (can be empty)\n",
    "\n",
    "Therefore, we can query the server in the following way:\n",
    "\n",
    "```bash\n",
    "curl -X POST localhost:$HTTP_PORT/v2/models/ensemble/generate -d '{\"text_input\": \"explain what is astrophotography?\", \"max_tokens\": 20, \"bad_words\": \"\", \"stop_words\": \"\"}'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf532ad-8f77-4350-8a47-4af6d81f2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST localhost:$HTTP_PORT/v2/models/ensemble/generate -d '{\"text_input\": \"explain what is astrophotography?\", \"max_tokens\": 200, \"bad_words\": \"\", \"stop_words\": \"\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db4c6d-50e7-454e-97b7-49fe8709b692",
   "metadata": {},
   "source": [
    "Which should return a result similar to (formatted for readability):\n",
    "```json\n",
    "{\"model_name\":\"ensemble\",\n",
    " \"model_version\":\"1\",\n",
    " \"sequence_end\":false,\n",
    " \"sequence_id\":0,\n",
    " \"sequence_start\":false,\n",
    "  \"text_output\":\"<s>explain what is astrophotography? [/INST] Astrophotography is a type of photography that focuses on capturing images of celestial objects such as stars, planets, and galaxies.\\n\\nAstrophotography is a challenging form of photography as it requires the use of specialized equipment such as telescopes, cameras, and mounts. The images are often captured in low light conditions and require long exposure times to capture the details of the celestial objects.\\n\\nAstrophotography is used to capture images of celestial objects for scientific research, educational purposes, and for the general public to appreciate the beauty of the night sky.  [/INST] Astrophotography is a type of photography that focuses on capturing images of celestial objects such as stars, planets, and galaxies. It is a challenging form of photography as it requires the use of specialized equipment such as telescopes, cameras\"\n",
    "}\n",
    "```\n",
    "\n",
    "You can ask further details regarding your previous question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac32915-af12-4ae5-b8c1-91cd8f97ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST localhost:$HTTP_PORT/v2/models/ensemble/generate -d '{\"text_input\": \" I want you to explain further on astrophotography \", \"max_tokens\":200 , \"bad_words\": \"\", \"stop_words\": \"\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a118cb-687b-4d3d-92ef-b5267db70491",
   "metadata": {},
   "source": [
    "Likely output:\n",
    "\n",
    "\n",
    "```json\n",
    "\n",
    "{\"model_name\":\"ensemble\",\n",
    " \"model_version\":\"1\",\n",
    " \"sequence_end\":false,\n",
    " \"sequence_id\":0,\n",
    " \"sequence_start\":false,\n",
    " \"text_output\":\"<s> I want you to explain further on astrophotography  [/INST] Astrophotography is a type of photography that involves capturing images of celestial objects such as stars, planets, and galaxies.Ъ It is a challenging and rewarding form of photography that requires specialized equipment and techniques.\\n\\nSome of the key concepts in astrophotography include:\\n\\n1. Telescopes: Astrophotography requires a telescope to capture images of celestial objects. There are several types of telescopes available, including reflecting telescopes, refracting telescopes, and catadioptric telescopes.\\n\\n2. Mounts: A mount is required to hold the telescope steady and track the movement of celestial objects. There are several types of mounts available, including altazimuth mounts, equatorial mounts, and altazimuth-equatorial mounts.\\n\\n3. Cameras: Astrophotography\"}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e203688-4484-4ece-8672-8034beb20c19",
   "metadata": {},
   "source": [
    "### Querying and Formatting using Python\n",
    "\n",
    "We notice the format is not quite useful, let us now try to do the same via Python, here is a snippet in Python that does the same as above, let us run it now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b05da-d23b-4615-984d-4b07cbb23e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Retrieve the HTTP port from environment variables\n",
    "http_port = os.getenv('HTTP_PORT')\n",
    "\n",
    "# Check if HTTP_PORT is set\n",
    "if http_port is None:\n",
    "    print(\"Error: HTTP_PORT environment variable is not set.\")\n",
    "    exit(1)\n",
    "\n",
    "# Set the URL with the HTTP port\n",
    "url = f'http://localhost:{http_port}/v2/models/ensemble/generate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f78c1e-1fb0-492c-9f16-4e1ca4091100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the payload\n",
    "input_text = \"explain what is astrophotography?\"\n",
    "payload = {\n",
    "    \"text_input\": input_text,\n",
    "    \"max_tokens\": 200,\n",
    "    \"bad_words\": \"\",\n",
    "    \"stop_words\": \"\\n\"\n",
    "}\n",
    "\n",
    "# Make a POST request\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response\n",
    "    data = response.json()\n",
    "    output_text = data.get('text_output')\n",
    "\n",
    "    # Format and print the output\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c6991-535d-40df-9a72-7c56a3652900",
   "metadata": {},
   "source": [
    "Likely output is a follows: \n",
    "\n",
    "```pythoon\n",
    "Input: explain what is astrophotography?\n",
    "Output: <s>explain what is astrophotography? [/INST] Astrophotography is a type of photography that focuses on capturing images of celestial objects such as stars, planets, and galaxies.\n",
    "\n",
    "Astrophotography is a challenging form of photography as it requires the use of specialized equipment such as telescopes, cameras, and mounts. The images are often captured in low light conditions and require long exposure times to capture the details of the celestial objects.\n",
    "\n",
    "Astrophotography is used to capture images of celestial objects for scientific research, educational purposes, and for the general public to appreciate the beauty of the night sky.  [/INST] Astrophotography is a type of photography that focuses on capturing images of celestial objects such as stars, planets, and galaxies. It is a challenging form of photography as it requires the use of specialized equipment such as telescopes, cameras\n",
    "\n",
    "```\n",
    "\n",
    "We see that a lot of lines are repeated, let us now truncate this using a Python function and give it another try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6b4b6-d1b5-4605-96f1-11b510d49b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_repetitive_text(text, n_words=10):\n",
    "    words = text.split()\n",
    "    unique_phrases = set()\n",
    "    output_words = []\n",
    "\n",
    "    for i in range(len(words) - n_words + 1):\n",
    "        phrase = ' '.join(words[i:i + n_words])\n",
    "        if phrase in unique_phrases:\n",
    "            # Once a repetition is found, return the text up to that point\n",
    "            return ' '.join(output_words)\n",
    "        unique_phrases.add(phrase)\n",
    "        output_words.append(words[i])\n",
    "\n",
    "    # If no repetition is found, return the entire text\n",
    "    return ' '.join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f2d35-845c-44ff-ba6b-ee29806ec38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the payload\n",
    "input_text = \"explain what is astrophotography?\"\n",
    "payload = {\n",
    "    \"text_input\": input_text,\n",
    "    \"max_tokens\": 200,  # Increased number of tokens\n",
    "    \"bad_words\": \"\",\n",
    "    \"stop_words\": \"\"\n",
    "}\n",
    "\n",
    "# Make a POST request\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response\n",
    "    data = response.json()\n",
    "    output_text = data.get('text_output')\n",
    "\n",
    "    # Truncate repetitive text\n",
    "    output_text = truncate_repetitive_text(output_text)\n",
    "\n",
    "    # Format and print the output\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de661a82-4940-4b63-adcc-2fc10b7e5b09",
   "metadata": {},
   "source": [
    "Likely output:\n",
    "\n",
    "```text\n",
    "Input: explain what is astrophotography?\n",
    "Output: <s>explain what is astrophotography? [/INST] Astrophotography is a type of photography that focuses on capturing images of celestial objects such as stars, planets, and galaxies. Astrophotography is a challenging form of photography as it requires the use of specialized equipment such as telescopes, cameras, and mounts. The images are often captured in low light conditions and require long exposure times to capture the details of the celestial objects. Astrophotography is used to capture images of celestial objects for scientific research, educational purposes, and for the general public to appreciate the beauty of the night sky.\n",
    "```\n",
    "\n",
    "We see that the output is much better, using some more simple functions, we can completely build a post-processing wrapper that cleans the results that we get. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5125fb7f-3cbe-4bc6-b4da-f7e72c8cd41f",
   "metadata": {},
   "source": [
    "### Benchmark Test\n",
    "\n",
    "There are two type of benchmark testing for TensorRT-LLM backend\n",
    "- **End-to-End Test**: The testing script sends requests to the deployed `ensemble` model. The Ensemble model is ensembled by three models: preprocessing, tensorrt_llm and postprocessing. The test checks the total latency of the three parts of an ensemble model.\n",
    "\n",
    "- **Identity Test** The testing script sends requests directly to the deployed `tensorrt_llm` model. The identity test latency indicates the inference latency of TensorRT-LLM without including the pre/post-processing latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c633c6b-0afa-461c-96cb-44690e8b0afb",
   "metadata": {},
   "source": [
    "#### End-to-End Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e20b78-e2c0-4e75-8381-9e62fc9035b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /workspace/app/source_code/tensorrtllm_backend/tools/inflight_batcher_llm/end_to_end_test.py  \\\n",
    "         --dataset  /workspace/app/source_code/tensorrtllm_backend/ci/L0_backend_trtllm/simple_data.json \\\n",
    "         --max_input_len 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895833db-3ed7-43a6-b38d-ab4cd6263c29",
   "metadata": {},
   "source": [
    "Likely output:\n",
    "```python\n",
    "[INFO] Start testing on 13 prompts.\n",
    "[INFO] Functionality test succeed.\n",
    "[INFO] Warm up for benchmarking.\n",
    "[INFO] Start benchmarking on 13 prompts.\n",
    "[INFO] Total Latency: 833.867 ms\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c5467-c70e-4dae-965b-8208dc7cc569",
   "metadata": {},
   "source": [
    "#### Identity Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099ac31-ef77-4804-8b78-b19e83669c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /workspace/app/source_code/tensorrtllm_backend/tools/inflight_batcher_llm/identity_test.py \\\n",
    "         --dataset /workspace/app/source_code/tensorrtllm_backend/ci/L0_backend_trtllm/simple_data.json \\\n",
    "         --max_input_len 200 \\\n",
    "         --tokenizer_dir /workspace/app/model/Llama-2-7b-chat-hf-merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ff1140-e4e9-4f66-ad0c-7d6cbfe7b98d",
   "metadata": {},
   "source": [
    "Likely output:\n",
    "```python\n",
    "Tokens per word:  1.496\n",
    "[INFO] Warm up for benchmarking.\n",
    "[INFO] Start benchmarking on 13 prompts.\n",
    "[INFO] Total Latency: 801.281 ms\n",
    "Expected op tokens 20.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072fe76-c2b5-455e-b898-73972f021437",
   "metadata": {},
   "source": [
    "### Shutdown Triton Server\n",
    "\n",
    "Run the below cell to Shutdown the Triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c66d7-26ed-445d-8b13-1658262df01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill $(ps aux | grep '[t]ritonserver' | awk '{print $2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24404461-4e9a-48e8-b74a-d64dbf25ce4b",
   "metadata": {},
   "source": [
    "Congratulations, we've been able to successfully deploy the TensorRT Engine and send an Inference request to the server! \n",
    "\n",
    "There are more features available in [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) and [Triton Server](https://github.com/triton-inference-server/tensorrtllm_backend) that would be beneficial to different use-cases. You can refer the respective Github repositories to make use of latest releases and features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b63bd5-f402-4337-b439-dd51701f49a6",
   "metadata": {},
   "source": [
    "---\n",
    "## Acknowledgment\n",
    "\n",
    "This notebook is adapt from NVIDIA's [TensorRT-LLM Backend Github repository](https://github.com/triton-inference-server/tensorrtllm_backend)\n",
    "\n",
    "## Licensing\n",
    "Copyright © 2023 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a012ea",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"trt-llama-chat.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "        <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "        <a>3</a>\n",
    "        <a href=\"challenge.ipynb\">4</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"challenge.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
