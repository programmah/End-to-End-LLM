{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98008ee2",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"triton-llama.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "        <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "        <a href=\"triton-llama.ipynb\">3</a>\n",
    "        <a>4</a>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6a61c",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "---\n",
    "\n",
    "This challenge notebook is vital to test your understanding and assist you in perfecting what you have learned in the previous notebooks. You are to provide a solution to the problem statement below by following the finetuning process, optimizing to generate tensorrt engine, and deploying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11285b6",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "eCommerce websites receive vast customer feedback through reviews and comments and sometimes have long chats with eCommerce agents. Manually analyzing this feedback is time-consuming, tedious, and prone to errors. The solution is to develop a generative AI-based solution that can efficiently analyze and summarize feedback and chat qualitatively.\n",
    "\n",
    "- **Use Case 1**: Customer feedback and Agent chat summarization\n",
    "- **Domain**: E-commerce\n",
    "- **Dataset**: Salesforce dialogstudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c2bc19",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "You are to reproduce the End-to-End approach using the `Salesforce dialogstudio` dataset. To complete the challenge, you are to implement the following steps:\n",
    "\n",
    "- Perform the finetuning process\n",
    "    - import needed libraries\n",
    "    - preprocess the dialogstudio TweetSumm (train, validation, test)\n",
    "    - set the path of your base model (Llama-2-7b)\n",
    "    - convert the model to Hugging Face Transformers Format\n",
    "    - initialize paths to the base model, tokenizer, and output checkpoint (`new_model='../../model/Llama-2-7b-hf-finetune'`)\n",
    "    - load tokenizer\n",
    "    - set the training parameter (TrainingArguments object)\n",
    "    - configure PEFT With LoRA (LoraConfig())\n",
    "    - 4-bit Quantization Configuration (BitsAndBytesConfig())\n",
    "    - load base model using AutoModelForCausalLM.from_pretrained()\n",
    "    - set the Trainer Parameter using SFTTrainer()\n",
    "    - apply trainer.train() and your new model to: `../../model/Llama-2-7b-hf-finetune`\n",
    "    - inference your model with provided test data\n",
    "    - merge the base model with the finetune checkpoint and save as `../../model/Llama-2-7b-hf-merged.` \n",
    "- Build TensorRT engine with single GPU and FP16\n",
    "    - execute build.py script to build tensorrt engine\n",
    "    - execute run.py script to run inference (use the flag `--input_text` or `--inpup_file` for the prompt text)\n",
    "- Deploy model on Triton server\n",
    "    - follow the instructions as given in the triton-llama.ipynb notebook\n",
    "\n",
    "The data preprocessing part of the solution code is written for you. You are to complete the rest by creating empty cells below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4deef1b",
   "metadata": {},
   "source": [
    "### Challenge Duration\n",
    "\n",
    "The challenge is expected to take  `3hrs: 30mins`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32180039",
   "metadata": {},
   "source": [
    "## Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4bd67",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d7e60a",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9742ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Salesforce/dialogstudio\",\"TweetSumm\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee60bdc",
   "metadata": {},
   "source": [
    "#### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\^[^ ]+\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@[^\\s]+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def transform_conversation(data):\n",
    "    transformed_text = \"\"\n",
    "    for row in data[\"log\"]:\n",
    "        user = format_text(row[\"user utterance\"])\n",
    "        transformed_text += f\"user: {user.strip()}\\n\"\n",
    "        agent = format_text(row[\"system response\"])\n",
    "        transformed_text += f\"agent: {agent.strip()}\\n\"\n",
    "    return transformed_text\n",
    "\n",
    "\n",
    "def format_training_prompt(conversation, summary):\n",
    "    return f\"\"\"### Instruction: Write  a summary of the conversation below. ### Input: {conversation.strip()} ### Response: {summary} \"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_conversation(data):\n",
    "    summaries = json.loads(data[\"original dialog info\"])[\"summaries\"][\"abstractive_summaries\"]\n",
    "    summary = summaries[0]\n",
    "    summary = \" \".join(summary)\n",
    "    \n",
    "    transformed_text = transform_conversation(data)\n",
    "    return {\n",
    "        \"conversation\": transformed_text,\n",
    "        \"summary\": summary,\n",
    "        \"text\": format_training_prompt(transformed_text, summary),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_example = generate_conversation(dataset[\"train\"][0])\n",
    "print(\"summary:\\n\",train_example[\"summary\"], \"\\n\")\n",
    "print(\"conversation:\\n\",train_example[\"conversation\"], \"\\n\")\n",
    "print(\"text:\\n\",train_example[\"text\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b4bc4",
   "metadata": {},
   "source": [
    "**Expected Output :**\n",
    "\n",
    "```text\n",
    "summary:\n",
    " Customer enquired about his Iphone and Apple watch which is not showing his any steps/activity and health activities. Agent is asking to move to DM and look into it. \n",
    "\n",
    "conversation:\n",
    " user: So neither my iPhone nor my Apple Watch are recording my steps/activity, and Health doesn’t recognise either source anymore for some reason. Any ideas?   please read the above.\n",
    "agent: Let’s investigate this together. To start, can you tell us the software versions your iPhone and Apple Watch are running currently?\n",
    "user: My iPhone is on 11.1.2, and my watch is on 4.1.\n",
    "agent: Thank you. Have you tried restarting both devices since this started happening?\n",
    "user: I’ve restarted both, also un-paired then re-paired the watch.\n",
    "agent: Got it. When did you first notice that the two devices were not talking to each other. Do the two devices communicate through other apps such as Messages?\n",
    "user: Yes, everything seems fine, it’s just Health and activity.\n",
    "agent: Let’s move to DM and look into this a bit more. When reaching out in DM, let us know when this first started happening please. For example, did it start after an update or after installing a certain app?\n",
    " \n",
    "\n",
    "text:\n",
    " ### Instruction: Write  a summary of the conversation below. ### Input: user: So neither my iPhone nor my Apple Watch are recording my steps/activity, and Health doesn’t recognise either source anymore for some reason. Any ideas?   please read the above.\n",
    "agent: Let’s investigate this together. To start, can you tell us the software versions your iPhone and Apple Watch are running currently?\n",
    "user: My iPhone is on 11.1.2, and my watch is on 4.1.\n",
    "agent: Thank you. Have you tried restarting both devices since this started happening?\n",
    "user: I’ve restarted both, also un-paired then re-paired the watch.\n",
    "agent: Got it. When did you first notice that the two devices were not talking to each other. Do the two devices communicate through other apps such as Messages?\n",
    "user: Yes, everything seems fine, it’s just Health and activity.\n",
    "agent: Let’s move to DM and look into this a bit more. When reaching out in DM, let us know when this first started happening please. For example, did it start after an update or after installing a certain app? ### Response: Customer enquired about his Iphone and Apple watch which is not showing his any steps/activity and health activities. Agent is asking to move to DM and look into it. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(data: Dataset):\n",
    "    return (\n",
    "         data.shuffle(seed=42).map(generate_conversation).remove_columns(\n",
    "         [\n",
    "          \"original dialog id\",\n",
    "             \"new dialog id\",\n",
    "             \"dialog index\",\n",
    "             \"original dialog info\",\n",
    "             \"log\",\n",
    "             \"prompt\",\n",
    "         ]\n",
    "         )\n",
    "    \n",
    "    )\n",
    "\n",
    "dataset[\"train\"] = process_dataset(dataset[\"train\"])\n",
    "dataset[\"validation\"] = process_dataset(dataset[\"validation\"])\n",
    "dataset[\"test\"] = process_dataset(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa2557",
   "metadata": {},
   "source": [
    "#### Download Llama-2-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Llama-2-7b model\n",
    "!python3 ../../source_code/Llama2/download-llama2.py\n",
    "\n",
    "print(\"extracting files......\")\n",
    "!tar -xf ../../model/Llama-2-7b.tar  -C ../../model\n",
    "\n",
    "print(\"files extraction done! removing tar file......\")\n",
    "!rm -rf ../../model/Llama-2-7b.tar\n",
    "print(\"All done!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f907533",
   "metadata": {},
   "source": [
    "### Complete the Rest Task in the Cell Below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c84500",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98e4bf",
   "metadata": {},
   "source": [
    "###  Run Inference\n",
    "\n",
    "The functions below format the test data for running inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd09fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_training_prompt(conversation, summary):\n",
    "    return f\"\"\"### Instruction: Write  a summary of the conversation below. ### Input: {conversation.strip()} ### Response: \"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881167b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_examples = []\n",
    "for row in dataset[\"test\"].select(range(3)):\n",
    "    #print(data_point)\n",
    "    prompt_examples.append(\n",
    "        {\n",
    "          \"summary\": row[\"summary\"],\n",
    "            \"conversation\": row[\"conversation\"],\n",
    "            \"prompt\": format_training_prompt(row[\"conversation\"], row[\"summary\"]),\n",
    "        }    \n",
    "    )\n",
    "    \n",
    "test_df = pd.DataFrame(prompt_examples)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d54e63",
   "metadata": {},
   "source": [
    "#### Create Custom Function to Summarize Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686271ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\" \n",
    "def summarize(model, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    inputs_length = len(inputs[\"input_ids\"][0])\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256, temperature= 1)\n",
    "    return tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b86609",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "\n",
    "example = test_df.iloc[0]\n",
    "print(\"Grundtruth : \\n\", example.summary)\n",
    "\n",
    "print(\"Conversation : \\n\", example.conversation)\n",
    "\n",
    "\n",
    "summary = summarize(model, example.prompt)\n",
    "print(\"All Prompt: \\n\", summary,\"\\n\")\n",
    "\n",
    "print(\"Summary Generated : \\n\",summary.strip().split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90d017",
   "metadata": {},
   "source": [
    "---\n",
    "## Switch to TensorRT-LLM Container to Continue the Challenge\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789bce5a",
   "metadata": {},
   "source": [
    "### Build TensorRT engine with single GPU and FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4275f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb2e99",
   "metadata": {},
   "source": [
    "### Deploy Model on Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892cb7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b52ee",
   "metadata": {},
   "source": [
    "---\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f005d89",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"triton-llama.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "        <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "        <a href=\"triton-llama.ipynb\">3</a>\n",
    "        <a>4</a>\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
